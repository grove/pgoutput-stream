# Feldera HTTP Connector Guide

This guide covers how to stream PostgreSQL logical replication changes directly to Feldera pipelines using the HTTP ingress connector.

## Table of Contents

- [Overview](#overview)
- [Table Name Mapping](#table-name-mapping)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [How It Works](#how-it-works)
- [Examples](#examples)
- [Multiple Targets](#multiple-targets)
- [Authentication](#authentication)
- [Error Handling](#error-handling)
- [Troubleshooting](#troubleshooting)
- [Best Practices](#best-practices)

## Overview

The Feldera HTTP connector enables direct streaming from PostgreSQL to Feldera pipelines via the HTTP ingress API with automatic type conversion. This integration allows you to:

- Build real-time data pipelines with streaming SQL
- Perform incremental view maintenance on PostgreSQL data
- Run streaming analytics and transformations
- Synchronize data between PostgreSQL and other systems
- Build event-driven architectures

**Key Features**:
- **Multi-table streaming**: Automatically routes changes from multiple PostgreSQL tables to corresponding Feldera tables
- **Schema-qualified naming**: Uses `schema_table` format to avoid naming conflicts across schemas
- **Type conversion**: Converts PostgreSQL data types to proper JSON types (integers → numbers, booleans → true/false, etc.)
- **Optional filtering**: Configure specific tables to stream, or stream all tables dynamically

## Table Name Mapping

PostgreSQL tables are mapped to Feldera using a **schema-qualified naming convention** to avoid conflicts:

| PostgreSQL Table | Feldera Table Name |
|------------------|--------------------|
| `public.users` | `public_users` |
| `public.orders` | `public_orders` |
| `analytics.users` | `analytics_users` |
| `sales.products` | `sales_products` |

**Why schema qualification?**
- Prevents naming conflicts when different schemas have tables with the same name
- Makes data lineage explicit in your Feldera pipeline
- Follows a predictable, deterministic naming pattern

**Example**: If your PostgreSQL has both `public.events` and `analytics.events`, they route to separate Feldera tables: `public_events` and `analytics_events`.

## Quick Start

### 1. Prerequisites

- PostgreSQL with logical replication enabled
- Feldera instance running (local or cloud)
- A Feldera pipeline with a table matching your PostgreSQL schema

### 2. Basic Example (All Tables)

```bash
# Build the tool
cargo build --release

# Stream all PostgreSQL tables to Feldera (dynamic routing)
./target/release/pgoutput-stream \
  --connection "host=localhost user=postgres dbname=mydb replication=database" \
  --slot feldera_slot \
  --publication my_pub \
  --format feldera \
  --target feldera \
  --feldera-url "http://localhost:8080" \
  --feldera-pipeline "postgres_cdc" \
  --create-slot
```

**Note**: Without `--feldera-tables`, all tables in the publication are automatically routed to Feldera using the schema_table naming convention.

### 3. Filtered Tables Example

```bash
# Stream only specific tables
./target/release/pgoutput-stream \
  --connection "host=localhost user=postgres dbname=mydb replication=database" \
  --slot feldera_slot \
  --publication my_pub \
  --format feldera \
  --target feldera \
  --feldera-url "http://localhost:8080" \
  --feldera-pipeline "postgres_cdc" \
  --feldera-tables "public_users,public_orders" \
  --create-slot
```

### 3. Verify in Feldera

Check the Feldera web UI or use the API to verify data is flowing:

```bash
# Query the pipeline status
curl http://localhost:8080/v0/pipelines/postgres_cdc

# Query table data (note: schema-qualified name)
curl http://localhost:8080/v0/pipelines/postgres_cdc/egress/public_users
curl http://localhost:8080/v0/pipelines/postgres_cdc/egress/public_orders
```

## Configuration

### Required Arguments

When `--target` includes `feldera`, these arguments are required:

| Argument | Description | Example |
|----------|-------------|---------|
| `--feldera-url` | Base URL of Feldera instance | `http://localhost:8080` |
| `--feldera-pipeline` | Name of the pipeline | `postgres_cdc` |

### Optional Arguments

| Argument | Description | Default |
|----------|-------------|---------|  
| `--feldera-tables` | Comma-separated list of schema-qualified tables (e.g., `public_users,public_orders`). If omitted, all tables are routed dynamically. | All tables |
{feldera-url}/v0/pipelines/{pipeline}/ingress/{table}?format=json&update_format=insert_delete&array=true
```

**Example:**
```
http://localhost:8080/v0/pipelines/postgres_cdc/ingress/users?format=json&update_format=insert_delete&array=true
```

**URL Parameters:**
- `format=json` - Expects JSON-formatted data
- `update_format=insert_delete` - Uses InsertDelete format (not raw updates)
- `array=true` - All events must be JSON arrays (required for batch operations)

## How It Works

### Event Conversion

PostgreSQL replication events are converted to Feldera InsertDelete format:

1. **INSERT** → Single-element array
   ```json
   [{"insert": {"id": 1, "name": "Alice", "email": "alice@example.com"}}]
   ```

2. **UPDATE** → Two-element array (delete old + insert new)
   ```json
   [
     {"delete": {"id": 1, "name": "Alice", "email": "alice@example.com"}},
     {"insert": {"id": 1, "name": "Alice", "email": "alice.updated@example.com"}}
   ]
   ```

3. **DELETE** → Single-element array
   ```json
   [{"delete": {"id": 1, "name": "Alice", "email": "alice@example.com"}}]
   ```

All events are sent as JSON arrays because the `array=true` parameter is used.

### Type Conversion

The connector automatically converts PostgreSQL values to appropriate JSON types:

| PostgreSQL Type | JSON Type | Example |
|----------------|-----------|----------|
| int2, int4, int8 (integers) | Number | `"id": 42` |
| float4, float8 (floats) | Number | `"price": 19.99` |
| numeric, decimal | Number | `"amount": 1000.50` |
| boolean | Boolean | `"active": true` |
| text, varchar, char | String | `"name": "Alice"` |
| timestamp, date, time | String | `"created_at": "2026-01-30 12:00:00"` |
| uuid, json, jsonb | String | Preserved as-is |

This ensures that Feldera can properly parse and type-check incoming data according to your pipeline's schema.

### Filtered Events

These PostgreSQL events are NOT sent to Feldera:
- `BEGIN` (transaction start)
- `COMMIT` (transaction end)
- `RELATION` (schema metadata)

Only data changes (INSERT, UPDATE, DELETE) are streamed.

### HTTP Request Details

- **Method**: POST
- **Content-Type**: application/json
- **Body**: JSON array of InsertDelete events (all operations wrapped in arrays)
- **Authentication**: Optional Bearer token via `--feldera-api-key`
- **Data Types**: Numeric fields sent as JSON numbers, not strings
- **Array Format**: The `array=true` parameter requires all events to be JSON arrays

### LSN Tracking and Acknowledgement

**Auto-Acknowledgement**: The tool uses `pg_logical_slot_get_binary_changes()` which **automatically confirms** LSNs as changes are retrieved from PostgreSQL:

- \u2705 **Automatic WAL advancement**: Confirmed flush LSN moves forward as events are fetched
- \u2705 **WAL cleanup**: Old WAL files are automatically cleaned up (no unbounded growth)
- \u26a0\ufe0f **At-least-once delivery**: If tool crashes after fetching but before Feldera confirms, events may be replayed
- \u26a0\ufe0f **No durability guarantee**: Tool trusts HTTP 200 response; doesn't verify Feldera persistence

**Monitoring LSN Progress**: Track replication slot status in PostgreSQL:

```sql
SELECT \n  slot_name,\n  confirmed_flush_lsn,\n  restart_lsn,\n  pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) AS lag\nFROM pg_replication_slots\nWHERE slot_name = 'your_slot_name';
```

**Graceful Shutdown**: On Ctrl+C, the tool prints:
- Last processed LSN
- Replication slot status (confirmed flush LSN, restart LSN, active state)

## Examples

### Example 1: Multi-Table Streaming (All Tables)

Stream all tables from PostgreSQL publication to Feldera:

```bash
pgoutput-stream \
  --connection "host=localhost user=postgres dbname=mydb replication=database" \
  --slot dev_slot \
  --publication all_tables \
  --format feldera \
  --target feldera \
  --feldera-url "http://localhost:8080" \
  --feldera-pipeline "dev_pipeline" \
  --create-slot
```

**Note**: All tables in the publication are automatically streamed. PostgreSQL `public.users` routes to Feldera `public_users`, `public.orders` routes to `public_orders`, etc.

### Example 2: Filtered Table Streaming

Stream only specific tables using the `--feldera-tables` filter:

```bash
pgoutput-stream \
  --connection "host=localhost user=postgres dbname=mydb replication=database" \
  --slot filtered_slot \
  --publication all_tables \
  --format feldera \
  --target feldera \
  --feldera-url "http://localhost:8080" \
  --feldera-pipeline "dev_pipeline" \
  --feldera-tables "public_users,public_orders" \
  --create-slot
```

**Note**: Only `public.users` and `public.orders` will be streamed. Other tables in the publication are skipped with a warning.

### Example 3: Complete Pipeline Setup

**Step 1: Set up PostgreSQL**

```sql
-- Enable logical replication in postgresql.conf
-- wal_level = logical

-- Create publication
CREATE PUBLICATION feldera_pub FOR TABLE users, orders;

-- Set replica identity for full row capture
ALTER TABLE users REPLICA IDENTITY FULL;
ALTER TABLE orders REPLICA IDENTITY FULL;
```

**Step 2: Create Feldera Pipeline**

```sql
-- In Feldera SQL editor
-- Note: Use schema_table naming convention
CREATE TABLE public_users (
    id INT,
    name VARCHAR,
    email VARCHAR,
    created_at TIMESTAMP
);

CREATE TABLE public_orders (
    id INT,
    user_id INT,
    product VARCHAR,
    amount DECIMAL(10,2),
    order_date TIMESTAMP
);

-- Example view
CREATE VIEW user_orders AS
SELECT 
    u.name,
    u.email,
    COUNT(o.id) as order_count,
    SUM(o.amount) as total_spent
FROM public_users u
LEFT JOIN public_orders o ON u.id = o.user_id
GROUP BY u.id, u.name, u.email;
```

**Step 3: Stream Data (Single Process)**

```bash
# Stream all tables from the publication
pgoutput-stream \
  --connection "host=localhost user=postgres dbname=mydb replication=database" \
  --slot feldera_slot \
  --publication feldera_pub \
  --format feldera \
  --target feldera \
  --feldera-url "http://localhost:8080" \
  --feldera-pipeline "my_pipeline" \
  --create-slot
```

**Advantage**: Single process streams both `users` and `orders` tables automatically. No need for multiple terminals or processes.

**Step 4: Query Results**

```bash
# Query the view in Feldera
curl http://localhost:8080/v0/pipelines/my_pipeline/egress/user_orders | jq
```

## Multiple Targets

Stream to Feldera and other targets simultaneously using comma-separated values:

### Feldera + Stdout

```bash
pgoutput-stream \
  --connection "..." \
  --slot my_slot \
  --publication my_pub \
  --format feldera \
  --target "stdout,feldera" \
  --feldera-url "http://localhost:8080" \
  --feldera-pipeline "my_pipeline"
```

### Feldera + NATS + Stdout

```bash
pgoutput-stream \
  --connection "..." \
  --slot my_slot \
  --publication my_pub \
  --format feldera \
  --target "stdout,nats,feldera" \
  --nats-server "nats://localhost:4222" \
  --feldera-url "http://localhost:8080" \
  --feldera-pipeline "my_pipeline"
```

This allows you to:
- Monitor events in real-time (stdout)
- Archive events for replay (NATS)
- Process events in streaming pipeline (Feldera)

## Authentication

### API Key Authentication

Feldera supports Bearer token authentication:

```bash
pgoutput-stream \
  --connection "..." \
  --slot my_slot \
  --publication my_pub \
  --format feldera \
  --target feldera \
  --feldera-url "https://secure.feldera.com" \
  --feldera-pipeline "my_pipeline" \
  --feldera-table "users" \
  --feldera-api-key "your-api-key-here"
```

### Environment Variables

For security, use environment variables:

```bash
export FELDERA_API_KEY="your-api-key-here"

pgoutput-stream \
  --connection "..." \
  --slot my_slot \
  --publication my_pub \
  --format feldera \
  --target feldera \
  --feldera-url "https://secure.feldera.com" \
  --feldera-pipeline "my_pipeline" \
  --feldera-table "users" \
  --feldera-api-key "${FELDERA_API_KEY}"
```

## Error Handling

The connector implements robust error handling:

### Connection Errors

If the Feldera server is unreachable:
```
Error: Failed to send data to Feldera: error sending request for url (...)
```

**Solutions:**
- Verify Feldera server is running
- Check network connectivity
- Verify firewall rules
- Check URL is correct (http:// or https://)

### Authentication Errors

HTTP 401 Unauthorized:
```
Error: Feldera ingress API returned error status 401: Unauthorized
```

**Solutions:**
- Verify API key is correct
- Check API key has necessary permissions
- Ensure API key is properly formatted

### Pipeline/Table Errors

HTTP 404 Not Found:
```
Error: Feldera ingress API returned error status 404: Not Found
```

**Solutions:**
- Verify pipeline exists and is running
- Check pipeline name for typos
- Verify table exists in pipeline SQL
- Ensure table name matches exactly (case-sensitive)

### Format Errors

HTTP 400 Bad Request - Type Mismatch:
```
Error: Feldera ingress API returned error status 400: error parsing field 'id': invalid type: string "1", expected i32
```

**Solutions:**
- This should not occur with the current version (automatic type conversion)
- If it does occur, check that RELATION metadata was received before data events
- Verify the replication stream started cleanly (use `--start-lsn` if resuming)

HTTP 400 Bad Request - Array Format:
```
Error: invalid type: map, expected a sequence
```

**Solutions:**
- Ensure the tool uses `array=true` parameter (automatic in current version)
- Verify Feldera API version supports array input format

HTTP 400 Bad Request - Schema Mismatch:
```
Error: Feldera ingress API returned error status 400: Invalid JSON format
```

**Solutions:**
- Verify PostgreSQL schema matches Feldera table schema
- Check column names match exactly (case-sensitive)
- Ensure data types are compatible
- Review Feldera logs for specific field errors

## Troubleshooting

### Common Issues

#### 1. No Data Flowing

**Symptom**: Tool runs but no data appears in Feldera

**Checks:**
- Verify publication includes the table: `SELECT * FROM pg_publication_tables WHERE pubname = 'my_pub';`
- Check replication slot is active: `SELECT * FROM pg_replication_slots WHERE slot_name = 'my_slot';`
- Ensure PostgreSQL changes are being made to published tables
- Verify pipeline is running in Feldera

#### 2. Type Mismatch Errors

**Symptom**: 400 errors about "invalid type: string, expected i32" or similar

**Causes:**
- RELATION metadata not received before data events
- Replication slot resumed from middle of stream
- Column type information missing

**Solutions:**
- Drop and recreate the replication slot with `--create-slot`
- Ensure the tool starts from the beginning or a known good LSN
- Verify PostgreSQL schema metadata is being replicated
- Check that all tables have REPLICA IDENTITY configured

#### 3. Schema Mismatch

**Symptom**: 400 errors about missing fields or unexpected data

**Solution:**
- Ensure PostgreSQL and Feldera table schemas match exactly
- Check column names (case-sensitive in Feldera)
- Verify data types are compatible
- Use `REPLICA IDENTITY FULL` for complete UPDATE events

#### 4. High Latency

**Symptom**: Delays between PostgreSQL changes and Feldera updates

**Checks:**
- Network latency between PostgreSQL and Feldera
- Feldera pipeline performance
- PostgreSQL replication lag: `SELECT * FROM pg_stat_replication;`
- Check for CPU/memory constraints

#### 5. Connection Drops

**Symptom**: Tool exits with connection errors

**Solutions:**
- Implement process supervision (systemd, supervisord)
- Check PostgreSQL connection limits
- Monitor network stability
- Review PostgreSQL logs for disconnection reasons

### Debug Mode

For detailed debugging, use stdout alongside Feldera:

```bash
pgoutput-stream \
  --connection "..." \
  --slot my_slot \
  --publication my_pub \
  --format feldera \
  --target "stdout,feldera" \
  --feldera-url "http://localhost:8080" \
  --feldera-pipeline "my_pipeline" \
  --feldera-table "users" \
  2>&1 | tee debug.log
```

This logs both:
- Events being sent (stdout)
- Error messages (stderr)

### Production Considerations

**Idempotency**: Feldera tables should handle duplicate events gracefully:
- Use primary keys or unique constraints
- Design views to be idempotent
- Consider using UPDATE format if Feldera supports it in the future

**Error Handling**: If Feldera HTTP request fails:
- Tool stops processing and returns error  
- LSN is already confirmed to PostgreSQL (fetched = acknowledged)
- Events are lost if tool crashes before successful HTTP delivery
- Consider implementing retry logic or using NATS as an intermediate buffer

**Monitoring**: Track these metrics:
- Replication lag (WAL position difference)
- HTTP error rates to Feldera
- Last processed LSN timestamp
- Feldera table row counts

## Best Practices

### 1. Use REPLICA IDENTITY FULL

Always set `REPLICA IDENTITY FULL` for complete UPDATE events:

```sql
ALTER TABLE users REPLICA IDENTITY FULL;
```

Without this, UPDATE events only include primary key in old tuple.

### 2. Monitor Replication Lag

Check PostgreSQL replication status:

```sql
SELECT slot_name, 
       confirmed_flush_lsn,
       pg_current_wal_lsn(),
       (pg_current_wal_lsn() - confirmed_flush_lsn) AS lag_bytes
FROM pg_replication_slots
WHERE slot_name = 'my_slot';
```

### 3. Handle Backpressure

If Feldera can't keep up:
- Add more Feldera workers
- Optimize pipeline queries
- Consider batching updates
- Use NATS as buffer

### 4. Schema Evolution

When changing schemas:
1. Stop the replication tool
2. Update both PostgreSQL and Feldera schemas
3. Restart with new schema

Alternatively, use separate slots for backward-compatible changes.

### 5. Handle At-Least-Once Delivery

The tool provides **at-least-once** delivery guarantees:

**Duplicate Prevention**:
- Use primary keys or unique constraints in Feldera tables
- Design idempotent views (e.g., use `MAX(timestamp)` instead of `COUNT(*)`)
- Test replay scenarios in development

**Data Loss Prevention**:
- Monitor HTTP error rates (errors = potential data loss)
- Consider NATS as an intermediate buffer for guaranteed delivery
- Implement alerting on prolonged failures
- Regularly verify data consistency between PostgreSQL and Feldera

**Recovery Strategy**:
```sql
-- If data loss detected, consider manual backfill
-- Check last successful LSN and compare with source tables
SELECT * FROM postgres_table WHERE updated_at > last_successful_timestamp;
```

### 6. High Availability

For production:
- Use process supervision (systemd)
- Monitor replication lag
- Set up alerting for errors
- Consider multiple Feldera instances
- Use connection pooling if needed

### 6. Testing

Test pipeline locally before production:
```bash
# Local Feldera
docker run -p 8080:8080 felderadb/feldera

# Run tool against test database
pgoutput-stream \
  --connection "host=localhost user=postgres dbname=test_db replication=database" \
  --slot test_slot \
  --publication test_pub \
  --format feldera \
  --target "stdout,feldera" \
  --feldera-url "http://localhost:8080" \
  --feldera-pipeline "test_pipeline" \
  --feldera-table "test_table" \
  --create-slot
```

### 7. Performance Tuning

**PostgreSQL:**
- `max_wal_senders = 10` (adjust based on slots)
- `max_replication_slots = 10`
- `wal_sender_timeout = 60s`

**Network:**
- Use same region/datacenter for low latency
- Consider dedicated network for replication traffic

**Feldera:**
- Tune worker count based on load
- Monitor pipeline metrics
- Optimize SQL queries in pipeline

## Related Documentation

- [FELDERA_FORMAT.md](FELDERA_FORMAT.md) - Details on InsertDelete format
- [README.md](README.md) - General usage and features
- [GETTING_STARTED.md](GETTING_STARTED.md) - Quick start guide
- [Feldera Documentation](https://www.feldera.com/docs/) - Official Feldera docs

## Support

For issues or questions:
- GitHub Issues: [Report a bug](https://github.com/yourusername/pgoutput-stream/issues)
- Feldera Community: [Feldera Discord/Forum]
- PostgreSQL Replication: [PostgreSQL Documentation](https://www.postgresql.org/docs/current/logical-replication.html)
